{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from toponetx import SimplicialComplex\n",
    "from topomodelx.nn.simplicial.hsn_layer import HSNLayer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create signal and domain\n",
    "\n",
    "The first step is to define the topological domain on which the TNN will operate, as well as the neighborhod structures characterizing this domain. We will only define th eneighborhood matrices that we plan on using.\n",
    "\n",
    "Here, we build a simple simplicial complex domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimplexView([(1,), (2,), (3,), (1, 2), (1, 3)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_set = [[1, 2], [1, 3]]\n",
    "\n",
    "domain = SimplicialComplex(edge_set)\n",
    "domain.simplices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we retrieve the boundary matrix (or incidence matrix) associated to the faces of this complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incidence_1\n",
      " [[-1. -1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]]\n",
      "adjacency_0\n",
      " [[0. 1. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "incidence_1 = domain.incidence_matrix(rank=1)\n",
    "adjacency_0 = domain.adjacency_matrix(rank=0)\n",
    "\n",
    "print(\"incidence_1\\n\", incidence_1.todense())\n",
    "print(\"adjacency_0\\n\", adjacency_0.todense())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each rank, the signal on this domain will look like a matrix with shape n_cells_of_rank_r x in_channels, where in_channels is the dimension of each cell's feature. In a a heterogenous domain, in_channels will vary by rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nodes = torch.tensor([[1.0, 1.0], [2.0, 2.0], [1.0, 1.0]])\n",
    "channels_nodes = x_nodes.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "incidence_1 = torch.from_numpy(incidence_1.todense()).to_sparse()\n",
    "adjacency_0 = torch.from_numpy(adjacency_0.todense()).to_sparse()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Neural Network\n",
    "\n",
    "Stack layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HSN(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            channels,\n",
    "            incidence_matrix_1,\n",
    "            adjacency_matrix_0,\n",
    "            initialization=\"xavier_uniform\",\n",
    "            n_layers=2):\n",
    "        super().__init__()\n",
    "        modules = []\n",
    "        for _ in range(n_layers):\n",
    "            print(\"append layer\")\n",
    "            modules.append(\n",
    "                HSNLayer(\n",
    "                    channels=channels,\n",
    "                    incidence_matrix_1=incidence_matrix_1,\n",
    "                    adjacency_matrix_0=adjacency_matrix_0,\n",
    "                    initialization=initialization,\n",
    "        ))\n",
    "        self.sequential = torch.nn.Sequential(*modules)\n",
    "        self.linear = torch.nn.Linear(channels, 1)\n",
    "\n",
    "    def forward(self, x_faces):\n",
    "        x_faces = self.sequential(x_faces)\n",
    "        return self.linear(x_faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append layer\n",
      "build messages\n",
      "append layer\n",
      "build messages\n",
      "tensor([[1., 1.],\n",
      "        [2., 2.],\n",
      "        [1., 1.]]) torch.Size([3, 2])\n",
      "forward HSNLayer\n",
      "weighted_x torch.Size([3, 2])\n",
      "neighborhood torch.Size([3, 3])\n",
      "weighted_x torch.Size([3, 2])\n",
      "neighborhood torch.Size([2, 3])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'_Merge' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(x_nodes, x_nodes\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> 8\u001b[0m nodes_pred_labels \u001b[39m=\u001b[39m model(x_nodes)\n\u001b[1;32m      9\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(nodes_pred_labels, nodes_gt_labels)\n\u001b[1;32m     10\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tmx/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m, in \u001b[0;36mHSN.forward\u001b[0;34m(self, x_faces)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x_faces):\n\u001b[0;32m---> 24\u001b[0m     x_faces \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msequential(x_faces)\n\u001b[1;32m     25\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(x_faces)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tmx/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tmx/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tmx/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/TopoModelX/topomodelx/nn/simplicial/hsn_layer.py:86\u001b[0m, in \u001b[0;36mHSNLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mforward HSNLayer\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m x_edges_and_nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlevel1(x)\n\u001b[0;32m---> 86\u001b[0m x_nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlevel2(x_edges_and_nodes)\n\u001b[1;32m     87\u001b[0m \u001b[39mreturn\u001b[39;00m x_nodes\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tmx/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/TopoModelX/topomodelx/base/level.py:33\u001b[0m, in \u001b[0;36mLevel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39m# the level is a split: sends one x into several roads\u001b[39;00m\n\u001b[1;32m     32\u001b[0m outputs \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 33\u001b[0m \u001b[39mfor\u001b[39;00m message_passing, x \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmessage_passings, x):\n\u001b[1;32m     34\u001b[0m     outputs\u001b[39m.\u001b[39mappend(message_passing(x))\n\u001b[1;32m     35\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "\u001b[0;31mTypeError\u001b[0m: '_Merge' object is not iterable"
     ]
    }
   ],
   "source": [
    "model = HSN(channels=channels_nodes, incidence_matrix_1=incidence_1, adjacency_matrix_0=adjacency_0, n_layers=2)\n",
    "nodes_gt_labels = torch.Tensor([[0], [1], [1]])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    print(x_nodes, x_nodes.shape)\n",
    "    nodes_pred_labels = model(x_nodes)\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(nodes_pred_labels, nodes_gt_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
