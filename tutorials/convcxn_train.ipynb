{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Set-up, create and train a convolutional CXN\n",
    "\n",
    "In this notebook, we create and train a simplified, non-attentional version of a CXN network, originally proposed in the paper by Hajij et. al : Cell Complex Neural Networks (https://arxiv.org/pdf/2010.00743.pdf). We will load a cellular complex dataset from the web and train the model to perform classificaiton on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "# import wandb\n",
    "\n",
    "from topomodelx.nn.cellular.convcxn_layer import ConvCXNLayer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU's are available, we will make use of them. Otherwise, this will run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "device = torch.device(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the hyperparameters for training. This sort of formulation can be useful for performing hyperparameter sweeps later on. Wandb is a useful tool for tracking sweeps -- feel free to uncomment the wandb code in order to launch your own experiment tracking with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--lr\", default=1e-3, type=float)\n",
    "parser.add_argument(\"--dataset\", default=\"shrec_16\", type=str)\n",
    "parser.add_argument(\"--hidden_dim\", default=64, type=int)\n",
    "parser.add_argument(\"--input_to_lin_layers\", \"-i\", default=32, type=int)\n",
    "parser.add_argument(\"--dropout\", default=0.1, type=float)\n",
    "parser.add_argument(\"--num_epochs\", default=3, type=int)\n",
    "parser.add_argument(\"--num_features\", default=50, type=int)\n",
    "parser.add_argument(\"--batch_size\", default=16, type=int)\n",
    "parser.add_argument(\"--with_rotation\", default=1, type=int, choices=[0, 1])\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "training_cfg = {\n",
    "    \"dataset\": args.dataset,\n",
    "    \"lr\": args.lr,\n",
    "    \"hidden_dim\": args.hidden_dim,\n",
    "    \"input_to_lin_layers\": args.input_to_lin_layers,\n",
    "    \"dropout\": args.dropout,\n",
    "    \"num_epochs\": args.num_epochs,\n",
    "}\n",
    "ds_name = args.dataset\n",
    "num_features = args.num_features\n",
    "rot = args.with_rotation\n",
    "cat = True\n",
    "num_classes = 5\n",
    "# wandb.login()\n",
    "# wandb.init(config=args, name=f\"ccnn_att{ds_name}_added_noise_lr_{args.lr}_hidden_{args.hidden_dim}\",project='shrec_16', entity='')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import data and neighborhood structures\n",
    "\n",
    "Now we load the train/test datasets from the web. The data object will contain both the cell features as well as the associate neighborhood matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data.Shrec16AugDataset(\n",
    "    root=\"./dataset\",\n",
    "    name=\"shrec_16\",\n",
    "    split=\"train\",\n",
    "    num_rot=1,\n",
    "    cat=True,\n",
    "    num_features=50,\n",
    ")\n",
    "test_dataset = data.Shrec16AugDataset(\n",
    "    root=\"./dataset\",\n",
    "    name=\"shrec_16\",\n",
    "    split=\"test\",\n",
    "    num_rot=1,\n",
    "    cat=True,\n",
    "    num_features=50,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=None, num_workers=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=None, num_workers=4, shuffle=False)\n",
    "data = train_dataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the loaded dataset, we extract the features on nodes/edges/faces, as well as their dimensions. We will need this for defining our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_ch_v 6 in_ch_e 50 in_ch_f 50\n"
     ]
    }
   ],
   "source": [
    "x_0 = data.x\n",
    "x_1 = data.x_e\n",
    "x_2 = data.x_f\n",
    "\n",
    "in_ch_0 = x_0.size(1)\n",
    "in_ch_1 = x_1.size(1)\n",
    "in_ch_2 = x_2.size(1)\n",
    "print(f\"in_ch_v {in_ch_0} in_ch_e {in_ch_1} in_ch_f {in_ch_2}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Neural Network\n",
    "\n",
    "Using the ConvCXNLayer class, we create a neural network with stacked layers. We define the amount of channels on the face and edge ranks to be different, making this a heterogenous network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvCXN(torch.nn.Module):\n",
    "    \"\"\"Convolutional CXN.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_ch_0 : int\n",
    "        Dimension of input features on nodes.\n",
    "    in_ch_1 : int\n",
    "        Dimension of input features on edges.\n",
    "    in_ch_2 : int\n",
    "        Dimension of input features on faces.\n",
    "    num_classes : int\n",
    "        Number of classes.\n",
    "    n_layers : int\n",
    "        Number of CXN layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch_0, in_ch_1, in_ch_2, num_classes, n_layers=2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(\n",
    "                ConvCXNLayer(\n",
    "                    in_channels_0=in_ch_0,\n",
    "                    in_channels_1=in_ch_1,\n",
    "                    in_channels_2=in_ch_2,\n",
    "                )\n",
    "            )\n",
    "        self.layers = layers\n",
    "        self.lin_0 = torch.nn.Linear(in_ch_0, num_classes)\n",
    "        self.lin_1 = torch.nn.Linear(in_ch_1, num_classes)\n",
    "        self.lin_2 = torch.nn.Linear(in_ch_2, num_classes)\n",
    "\n",
    "    def forward(self, x_0, x_1, neighborhood_0_to_0, neighborhood_1_to_2):\n",
    "        \"\"\"Forward computation through ConvCXN layers then linear layers.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x_0, x_1, x_2 = layer(x_0, x_1, neighborhood_0_to_0, neighborhood_1_to_2)\n",
    "        x_0 = self.lin_0(x_0)\n",
    "        x_1 = self.lin_1(x_1)\n",
    "        x_2 = self.lin_2(x_2)\n",
    "        return torch.mean(x_2, dim=0) + torch.mean(x_1, dim=0) + torch.mean(x_0, dim=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We specify the model, initialize loss, and specify an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvCXN(in_ch_0, in_ch_1, in_ch_2, num_classes=num_classes, n_layers=2)\n",
    "\n",
    "# wandb.watch(model, log_freq=len(train_dataset))\n",
    "model = model.to(device)\n",
    "crit = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell performs the training, looping over the network for 5 epochs and testing after every 2 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 2.0716 Train_acc: 0.2000\n",
      "Epoch: 2 loss: 1.7892 Train_acc: 0.2000\n",
      "Test_acc: 0.2000\n",
      "Epoch: 3 loss: 1.6736 Train_acc: 0.2250\n"
     ]
    }
   ],
   "source": [
    "test_interval = 2\n",
    "for epoch_i in range(1, args.num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    num_samples = 0\n",
    "    correct = 0\n",
    "    model.train()\n",
    "    for batch_i, data in enumerate(train_loader, start=1):\n",
    "        opt.zero_grad()\n",
    "        x_0 = data.x.to(device)\n",
    "        x_1 = data.x_e.to(device)\n",
    "\n",
    "        y = data.y.to(device)\n",
    "\n",
    "        A0 = data.A0.to_sparse().to(device)\n",
    "        B2T = data.B2T.to_sparse().to(device)\n",
    "\n",
    "        y_hat = model(x_0.float(), x_1.float(), A0.float(), B2T.float())\n",
    "\n",
    "        loss = crit(y_hat, y)\n",
    "        correct += (y_hat.argmax() == y).sum().item()\n",
    "        num_samples += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        print(f\"Done {batch_i}/{len(train_dataset)}\", end=\"\\r\")\n",
    "        epoch_loss.append(loss.item())\n",
    "    train_acc = correct / num_samples\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f} Train_acc: {train_acc:.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    # wandb.log({\"loss\": np.mean(epoch_loss), \"Train_acc\": train_acc, \"epoch\": epoch_i})\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            num_samples = 0\n",
    "            correct = 0\n",
    "            for batch_i, data in enumerate(test_loader, start=1):\n",
    "                x_0 = data.x.to(device)\n",
    "                x_1 = data.x_e.to(device)\n",
    "                y = data.y.to(device)\n",
    "\n",
    "                A0 = data.A0.to_sparse().to(device)\n",
    "                B2T = data.B2T.to_sparse().to(device)\n",
    "\n",
    "                y_hat = model(x_0.float(), x_1.float(), A0.float(), B2T.float())\n",
    "\n",
    "                correct += (y_hat.argmax() == y).sum().item()\n",
    "                num_samples += 1\n",
    "                print(f\"Done {batch_i}/{len(test_dataset)}\", end=\"\\r\")\n",
    "            test_acc = correct / num_samples\n",
    "            print(f\"Test_acc: {test_acc:.4f}\", flush=True)\n",
    "            # wandb.log({\"Test_acc\": test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
