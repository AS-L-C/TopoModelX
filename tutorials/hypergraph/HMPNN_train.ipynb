{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Hypergraph Message Passing Neural Network (HMPNN)\n",
    "\n",
    "In this notebook, we will create and train a Hypergraph Message Passing Neural Network in the hypergraph domain. This method is introduced in the paper [Message Passing Neural Networks for\n",
    "Hypergraphs](https://arxiv.org/abs/2203.16995) by Heydari et Livi 2022. We will use a benchmark dataset, Cora, a collection of 2708 academic papers and 5429 citation relations, to do the task of node classification. There are 7 category labels, namely `Case_Based`, `Genetic_Algorithms`, `Neural_Networks`, `Probabilistic_Methods`, `Reinforcement_Learning`, `Rule_Learning` and `Theory`.\n",
    "\n",
    "Each document is initially represented as a binary vector of length 1433, standing for a unique subset of the words within the papers, in which a value of 1 means the presence of its corresponding word in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:51.222779223Z",
     "start_time": "2023-06-01T16:14:49.575421023Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from topomodelx.nn.hypergraph.HMPNN_layer import HMPNNLayer\n",
    "from typing import List"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU's are available, we will make use of them. Otherwise, this will run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:51.959770754Z",
     "start_time": "2023-06-01T16:14:51.956096841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "Here we download the dataset. It contains initial representation of nodes, the adjacency information and the category labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
    "! tar -xf cora.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_ids: List[str] = []\n",
    "node_labels = []\n",
    "node_features: List[torch.Tensor] = []\n",
    "with open(\"cora/cora.content\") as f:\n",
    "    for line in f:\n",
    "        entries = line.split()\n",
    "        node_ids.append(entries[0])\n",
    "        node_labels.append(entries[-1])\n",
    "        node_features.append(\n",
    "            torch.tensor(list(map(int, entries[1:-1])), dtype=torch.float)\n",
    "        )\n",
    "node_labels = np.array(node_labels)\n",
    "node_features = torch.stack(node_features)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(node_labels)\n",
    "node_indices = np.arange(len(node_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we construct the incidence matrix ($B_1$) which is of shape $n_\\text{nodes} \\times n_\\text{edges}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "incidence_1 = torch.zeros((len(node_ids), len(node_ids)), dtype=torch.long)\n",
    "nodeid_to_index_map = dict(zip(node_ids, node_indices))\n",
    "node_hyperedge_list = []\n",
    "with open(\"cora/cora.cites\") as f:\n",
    "    for line in f:\n",
    "        cited_id, citing_id = line.split()\n",
    "        cited_index = nodeid_to_index_map[cited_id]\n",
    "        citing_index = nodeid_to_index_map[citing_id]\n",
    "        node_hyperedge_list.append([citing_index, cited_index])\n",
    "        node_hyperedge_list.append([cited_index, citing_index])\n",
    "incidence_1 = torch.sparse_coo_tensor(\n",
    "    torch.LongTensor(node_hyperedge_list).T,\n",
    "    torch.ones(len(node_hyperedge_list)),\n",
    "    dtype=torch.long,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Data(x=node_features, incidence_1=incidence_1, y=torch.from_numpy(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her we split the data into train, validation and test splits according to the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adopted from torch_geometric.transforms.RandomNodeSplit with proper modification.\n",
    "num_classes = int(dataset[\"y\"].max().item()) + 1\n",
    "train_mask = torch.zeros(len(node_ids), dtype=torch.bool)\n",
    "val_mask = torch.zeros(len(node_ids), dtype=torch.bool)\n",
    "test_mask = torch.zeros(len(node_ids), dtype=torch.bool)\n",
    "for c in range(num_classes):\n",
    "    idx = (dataset[\"y\"] == c).nonzero(as_tuple=False).view(-1)\n",
    "    idx = idx[torch.randperm(idx.size(0))]\n",
    "\n",
    "    train_idx = idx[:20]\n",
    "    train_mask[train_idx] = True\n",
    "\n",
    "    val_idx = idx[20:90]\n",
    "    val_mask[val_idx] = True\n",
    "\n",
    "    test_idx = idx[90:]\n",
    "    test_mask[test_idx] = True\n",
    "\n",
    "dataset.node_stores[0].train_mask = train_mask\n",
    "dataset.node_stores[0].val_mask = val_mask\n",
    "dataset.node_stores[0].test_mask = test_mask\n",
    "\n",
    "dataset = dataset.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Neural Network\n",
    "\n",
    "Using the `HMPNNLayer` class, we create a neural network with stacked layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:56.033274119Z",
     "start_time": "2023-06-01T16:14:56.029056913Z"
    }
   },
   "outputs": [],
   "source": [
    "class HMPNN(torch.nn.Module):\n",
    "    \"\"\"Neural network implementation of HMPNN\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    in_features : int\n",
    "        Dimension of input features\n",
    "    hidden_features : Tuple[int]\n",
    "        A tuple of hidden feature dimensions to gradually reduce node/hyperedge representations feature\n",
    "        dimension from in_features to the last item in the tuple.\n",
    "    num_classes: int\n",
    "        Number of classes\n",
    "    n_layer : 2\n",
    "        Number of HMPNNLayer layers.\n",
    "    adjacency_dropout_rate: 0.7\n",
    "        Adjacency dropout rate.\n",
    "    regular_dropout_rate: 0.5\n",
    "        Regular dropout rate applied on features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        hidden_features,\n",
    "        num_classes,\n",
    "        n_layer=2,\n",
    "        adjacency_dropout_rate=0.7,\n",
    "        regular_dropout_rate=0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_features = (in_features,) + hidden_features\n",
    "        self.to_hidden_linear = torch.nn.Sequential(\n",
    "            *[\n",
    "                torch.nn.Linear(hidden_features[i], hidden_features[i + 1])\n",
    "                for i in range(len(hidden_features) - 1)\n",
    "            ]\n",
    "        )\n",
    "        self.layers = torch.nn.ModuleList(\n",
    "            [\n",
    "                HMPNNLayer(\n",
    "                    hidden_features[-1],\n",
    "                    adjacency_dropout=adjacency_dropout_rate,\n",
    "                    updating_dropout=regular_dropout_rate,\n",
    "                )\n",
    "                for _ in range(n_layer)\n",
    "            ]\n",
    "        )\n",
    "        self.to_categories_linear = torch.nn.Linear(hidden_features[-1], num_classes)\n",
    "\n",
    "    def forward(self, x_0, x_1, incidence_1):\n",
    "        \"\"\"Forward computation through layers.\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        x_0 : torch.Tensor\n",
    "            Node features with shape [n_nodes, in_features]\n",
    "        x_1 : torch.Tensor\n",
    "            Hyperedge features with shape [n_hyperedges, in_features]\n",
    "        incidence_1: torch.sparse.Tensor\n",
    "            Incidence matrix (B1) of shape [n_nodes, n_hyperedges]\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        y_pred : torch.Tensor\n",
    "            Predicted logits with shape [n_nodes, num_classes]\n",
    "        \"\"\"\n",
    "        x_0 = self.to_hidden_linear(x_0)\n",
    "        x_1 = self.to_hidden_linear(x_1)\n",
    "        for layer in self.layers:\n",
    "            x_0, x_1 = layer(x_0, x_1, incidence_1)\n",
    "\n",
    "        return self.to_categories_linear(x_0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We then specify the hyperparameters and construct the model, the loss and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:58.153514385Z",
     "start_time": "2023-06-01T16:14:57.243596119Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(41)\n",
    "\n",
    "in_features = 1433\n",
    "hidden_features = 8\n",
    "num_classes = 7\n",
    "n_layers = 2\n",
    "\n",
    "model = HMPNN(in_features, (256, hidden_features), num_classes, n_layers).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train the model, looping over the network for a low amount of epochs. We keep training minimal for the purpose of rapid testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:15:01.683216142Z",
     "start_time": "2023-06-01T16:15:00.727075750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 train loss: 2.1276 train acc: 0.14 val loss: 2.1094 val acc: 0.14\n",
      "Epoch: 2 train loss: 2.0316 train acc: 0.14 val loss: 2.0689 val acc: 0.14\n",
      "Epoch: 3 train loss: 1.9684 train acc: 0.15 val loss: 2.0353 val acc: 0.14\n",
      "Epoch: 4 train loss: 1.9527 train acc: 0.15 val loss: 2.0091 val acc: 0.14\n",
      "Epoch: 5 train loss: 1.9251 train acc: 0.19 val loss: 1.9861 val acc: 0.14\n",
      "Epoch: 6 train loss: 1.9151 train acc: 0.17 val loss: 1.9669 val acc: 0.14\n",
      "Epoch: 7 train loss: 1.8849 train acc: 0.29 val loss: 1.9513 val acc: 0.14\n",
      "Epoch: 8 train loss: 1.8785 train acc: 0.29 val loss: 1.9383 val acc: 0.15\n",
      "Epoch: 9 train loss: 1.8600 train acc: 0.30 val loss: 1.9272 val acc: 0.15\n",
      "Epoch: 10 train loss: 1.8551 train acc: 0.25 val loss: 1.9179 val acc: 0.19\n",
      "Epoch: 11 train loss: 1.8443 train acc: 0.36 val loss: 1.9096 val acc: 0.26\n",
      "Epoch: 12 train loss: 1.8385 train acc: 0.34 val loss: 1.9020 val acc: 0.30\n",
      "Epoch: 13 train loss: 1.8267 train acc: 0.36 val loss: 1.8951 val acc: 0.28\n",
      "Epoch: 14 train loss: 1.8347 train acc: 0.37 val loss: 1.8882 val acc: 0.28\n",
      "Epoch: 15 train loss: 1.8101 train acc: 0.39 val loss: 1.8816 val acc: 0.29\n",
      "Epoch: 16 train loss: 1.8012 train acc: 0.39 val loss: 1.8755 val acc: 0.31\n",
      "Epoch: 17 train loss: 1.7980 train acc: 0.39 val loss: 1.8689 val acc: 0.33\n",
      "Epoch: 18 train loss: 1.7788 train acc: 0.41 val loss: 1.8620 val acc: 0.33\n",
      "Epoch: 19 train loss: 1.7915 train acc: 0.44 val loss: 1.8546 val acc: 0.34\n",
      "Epoch: 20 train loss: 1.7690 train acc: 0.43 val loss: 1.8466 val acc: 0.35\n",
      "Epoch: 21 train loss: 1.7607 train acc: 0.48 val loss: 1.8382 val acc: 0.36\n",
      "Epoch: 22 train loss: 1.7597 train acc: 0.45 val loss: 1.8295 val acc: 0.36\n",
      "Epoch: 23 train loss: 1.7505 train acc: 0.43 val loss: 1.8192 val acc: 0.36\n",
      "Epoch: 24 train loss: 1.7322 train acc: 0.51 val loss: 1.8082 val acc: 0.37\n",
      "Epoch: 25 train loss: 1.7266 train acc: 0.50 val loss: 1.7971 val acc: 0.37\n",
      "Epoch: 26 train loss: 1.7108 train acc: 0.50 val loss: 1.7874 val acc: 0.38\n",
      "Epoch: 27 train loss: 1.7185 train acc: 0.49 val loss: 1.7764 val acc: 0.38\n",
      "Epoch: 28 train loss: 1.6917 train acc: 0.46 val loss: 1.7663 val acc: 0.39\n",
      "Epoch: 29 train loss: 1.6961 train acc: 0.51 val loss: 1.7569 val acc: 0.39\n",
      "Epoch: 30 train loss: 1.6709 train acc: 0.53 val loss: 1.7475 val acc: 0.40\n",
      "Epoch: 31 train loss: 1.6791 train acc: 0.53 val loss: 1.7381 val acc: 0.40\n",
      "Epoch: 32 train loss: 1.6603 train acc: 0.50 val loss: 1.7320 val acc: 0.39\n",
      "Epoch: 33 train loss: 1.6428 train acc: 0.51 val loss: 1.7258 val acc: 0.39\n",
      "Epoch: 34 train loss: 1.6437 train acc: 0.51 val loss: 1.7164 val acc: 0.40\n",
      "Epoch: 35 train loss: 1.6276 train acc: 0.51 val loss: 1.7040 val acc: 0.40\n",
      "Epoch: 36 train loss: 1.6274 train acc: 0.54 val loss: 1.6873 val acc: 0.40\n",
      "Epoch: 37 train loss: 1.6107 train acc: 0.52 val loss: 1.6731 val acc: 0.41\n",
      "Epoch: 38 train loss: 1.5979 train acc: 0.52 val loss: 1.6603 val acc: 0.42\n",
      "Epoch: 39 train loss: 1.5850 train acc: 0.55 val loss: 1.6477 val acc: 0.41\n",
      "Epoch: 40 train loss: 1.6203 train acc: 0.54 val loss: 1.6377 val acc: 0.42\n",
      "Epoch: 41 train loss: 1.5556 train acc: 0.58 val loss: 1.6290 val acc: 0.43\n",
      "Epoch: 42 train loss: 1.5692 train acc: 0.62 val loss: 1.6243 val acc: 0.44\n",
      "Epoch: 43 train loss: 1.5280 train acc: 0.59 val loss: 1.6268 val acc: 0.44\n",
      "Epoch: 44 train loss: 1.5508 train acc: 0.59 val loss: 1.6249 val acc: 0.44\n",
      "Epoch: 45 train loss: 1.5164 train acc: 0.60 val loss: 1.6217 val acc: 0.45\n",
      "Epoch: 46 train loss: 1.5180 train acc: 0.55 val loss: 1.6077 val acc: 0.46\n",
      "Epoch: 47 train loss: 1.4640 train acc: 0.65 val loss: 1.5893 val acc: 0.46\n",
      "Epoch: 48 train loss: 1.4855 train acc: 0.56 val loss: 1.5767 val acc: 0.48\n",
      "Epoch: 49 train loss: 1.4756 train acc: 0.61 val loss: 1.5674 val acc: 0.49\n",
      "Epoch: 50 train loss: 1.4370 train acc: 0.61 val loss: 1.5602 val acc: 0.49\n",
      "Epoch: 51 train loss: 1.4557 train acc: 0.61 val loss: 1.5561 val acc: 0.48\n",
      "Epoch: 52 train loss: 1.4482 train acc: 0.63 val loss: 1.5559 val acc: 0.49\n",
      "Epoch: 53 train loss: 1.4056 train acc: 0.56 val loss: 1.5542 val acc: 0.50\n",
      "Epoch: 54 train loss: 1.4095 train acc: 0.61 val loss: 1.5396 val acc: 0.51\n",
      "Epoch: 55 train loss: 1.3908 train acc: 0.64 val loss: 1.5238 val acc: 0.51\n",
      "Epoch: 56 train loss: 1.3819 train acc: 0.69 val loss: 1.5142 val acc: 0.51\n",
      "Epoch: 57 train loss: 1.4066 train acc: 0.59 val loss: 1.5081 val acc: 0.52\n",
      "Epoch: 58 train loss: 1.3534 train acc: 0.69 val loss: 1.5054 val acc: 0.51\n",
      "Epoch: 59 train loss: 1.3555 train acc: 0.63 val loss: 1.5245 val acc: 0.49\n",
      "Epoch: 60 train loss: 1.3549 train acc: 0.70 val loss: 1.5401 val acc: 0.47\n",
      "Epoch: 61 train loss: 1.2830 train acc: 0.74 val loss: 1.5371 val acc: 0.47\n",
      "Epoch: 62 train loss: 1.3013 train acc: 0.74 val loss: 1.5196 val acc: 0.48\n",
      "Epoch: 63 train loss: 1.2784 train acc: 0.68 val loss: 1.4989 val acc: 0.50\n",
      "Epoch: 64 train loss: 1.2638 train acc: 0.66 val loss: 1.4833 val acc: 0.50\n",
      "Epoch: 65 train loss: 1.2540 train acc: 0.69 val loss: 1.4734 val acc: 0.51\n",
      "Epoch: 66 train loss: 1.2345 train acc: 0.74 val loss: 1.4734 val acc: 0.50\n",
      "Epoch: 67 train loss: 1.2185 train acc: 0.68 val loss: 1.4731 val acc: 0.50\n",
      "Epoch: 68 train loss: 1.2306 train acc: 0.70 val loss: 1.4685 val acc: 0.50\n",
      "Epoch: 69 train loss: 1.2636 train acc: 0.71 val loss: 1.4771 val acc: 0.50\n",
      "Epoch: 70 train loss: 1.1783 train acc: 0.73 val loss: 1.4803 val acc: 0.50\n",
      "Epoch: 71 train loss: 1.1959 train acc: 0.69 val loss: 1.4704 val acc: 0.50\n",
      "Epoch: 72 train loss: 1.1773 train acc: 0.71 val loss: 1.4628 val acc: 0.48\n",
      "Epoch: 73 train loss: 1.1580 train acc: 0.76 val loss: 1.4511 val acc: 0.49\n",
      "Epoch: 74 train loss: 1.1354 train acc: 0.75 val loss: 1.4488 val acc: 0.50\n",
      "Epoch: 75 train loss: 1.1619 train acc: 0.72 val loss: 1.4602 val acc: 0.49\n",
      "Epoch: 76 train loss: 1.1369 train acc: 0.74 val loss: 1.4741 val acc: 0.48\n",
      "Epoch: 77 train loss: 1.1195 train acc: 0.78 val loss: 1.4342 val acc: 0.50\n",
      "Epoch: 78 train loss: 1.1261 train acc: 0.74 val loss: 1.4152 val acc: 0.49\n",
      "Epoch: 79 train loss: 1.0622 train acc: 0.74 val loss: 1.4156 val acc: 0.50\n",
      "Epoch: 80 train loss: 1.0938 train acc: 0.79 val loss: 1.4152 val acc: 0.50\n",
      "Epoch: 81 train loss: 1.0650 train acc: 0.71 val loss: 1.4010 val acc: 0.52\n",
      "Epoch: 82 train loss: 1.1122 train acc: 0.71 val loss: 1.4122 val acc: 0.51\n",
      "Epoch: 83 train loss: 1.0467 train acc: 0.71 val loss: 1.4500 val acc: 0.50\n",
      "Epoch: 84 train loss: 0.9841 train acc: 0.79 val loss: 1.4990 val acc: 0.48\n",
      "Epoch: 85 train loss: 1.0030 train acc: 0.78 val loss: 1.5262 val acc: 0.47\n",
      "Epoch: 86 train loss: 1.0751 train acc: 0.79 val loss: 1.5601 val acc: 0.47\n",
      "Epoch: 87 train loss: 1.0167 train acc: 0.82 val loss: 1.4581 val acc: 0.51\n",
      "Epoch: 88 train loss: 1.0019 train acc: 0.79 val loss: 1.4001 val acc: 0.54\n",
      "Epoch: 89 train loss: 0.9708 train acc: 0.78 val loss: 1.3749 val acc: 0.54\n",
      "Epoch: 90 train loss: 0.9798 train acc: 0.83 val loss: 1.4086 val acc: 0.52\n",
      "Epoch: 91 train loss: 0.9684 train acc: 0.75 val loss: 1.4029 val acc: 0.52\n",
      "Epoch: 92 train loss: 0.9563 train acc: 0.81 val loss: 1.3832 val acc: 0.52\n",
      "Epoch: 93 train loss: 0.9498 train acc: 0.81 val loss: 1.3831 val acc: 0.52\n",
      "Epoch: 94 train loss: 0.9573 train acc: 0.77 val loss: 1.4066 val acc: 0.53\n",
      "Epoch: 95 train loss: 0.8926 train acc: 0.81 val loss: 1.4465 val acc: 0.51\n",
      "Epoch: 96 train loss: 0.9473 train acc: 0.81 val loss: 1.4754 val acc: 0.51\n",
      "Epoch: 97 train loss: 0.9169 train acc: 0.78 val loss: 1.4351 val acc: 0.52\n",
      "Epoch: 98 train loss: 0.9288 train acc: 0.79 val loss: 1.3983 val acc: 0.54\n",
      "Epoch: 99 train loss: 0.8961 train acc: 0.84 val loss: 1.3912 val acc: 0.53\n",
      "Epoch: 100 train loss: 0.8753 train acc: 0.83 val loss: 1.3903 val acc: 0.52\n"
     ]
    }
   ],
   "source": [
    "train_y_true = dataset[\"y\"][dataset[\"train_mask\"]]\n",
    "val_y_true = dataset[\"y\"][dataset[\"val_mask\"]]\n",
    "initial_x_1 = torch.zeros_like(dataset[\"x\"])\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred_logits = model(dataset[\"x\"], initial_x_1, dataset[\"incidence_1\"])\n",
    "    loss = loss_fn(y_pred_logits[dataset[\"train_mask\"]], train_y_true)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_loss = loss.item()\n",
    "    y_pred = y_pred_logits.argmax(dim=-1)\n",
    "    train_acc = accuracy_score(train_y_true, y_pred[dataset[\"train_mask\"]])\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_logits = model(dataset[\"x\"], initial_x_1, dataset[\"incidence_1\"])\n",
    "    val_loss = loss_fn(y_pred_logits[dataset[\"val_mask\"]], val_y_true).item()\n",
    "    y_pred = y_pred_logits.argmax(dim=-1)\n",
    "    val_acc = accuracy_score(val_y_true, y_pred[dataset[\"val_mask\"]])\n",
    "    print(\n",
    "        f\"Epoch: {epoch + 1} train loss: {train_loss:.4f} train acc: {train_acc:.2f} \"\n",
    "        f\"val loss: {val_loss:.4f} val acc: {val_acc:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the model against the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.3764 test acc: 0.54 \n"
     ]
    }
   ],
   "source": [
    "test_y_true = dataset[\"y\"][dataset[\"test_mask\"]]\n",
    "initial_x_1 = torch.zeros_like(dataset[\"x\"])\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_logits = model(dataset[\"x\"], initial_x_1, dataset[\"incidence_1\"])\n",
    "test_loss = loss_fn(y_pred_logits[dataset[\"test_mask\"]], test_y_true).item()\n",
    "y_pred = y_pred_logits.argmax(dim=-1)\n",
    "test_acc = accuracy_score(test_y_true, y_pred[dataset[\"test_mask\"]])\n",
    "print(f\"Test loss: {test_loss:.4f} test acc: {test_acc:.2f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
