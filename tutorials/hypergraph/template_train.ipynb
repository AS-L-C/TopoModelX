{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Hypergraph Neural Network\n",
    "\n",
    "In this notebook, we will create and train a two-step message passing network in the hypergraph domain. We will use a benchmark dataset, MUTAG (from the TUDataset), to train the model to perform binary classification at the level of the h. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:51.222779223Z",
     "start_time": "2023-06-01T16:14:49.575421023Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:51.222779223Z",
     "start_time": "2023-06-01T16:14:49.575421023Z"
    }
   },
   "outputs": [],
   "source": [
    "from toponetx import SimplicialComplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:51.222779223Z",
     "start_time": "2023-06-01T16:14:49.575421023Z"
    }
   },
   "outputs": [],
   "source": [
    "import toponetx.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:51.222779223Z",
     "start_time": "2023-06-01T16:14:49.575421023Z"
    }
   },
   "outputs": [],
   "source": [
    "from topomodelx.nn.hypergraph.template_layer import TemplateLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU's are available, we will make use of them. Otherwise, this will run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:51.959770754Z",
     "start_time": "2023-06-01T16:14:51.956096841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import data ##\n",
    "\n",
    "The first step is to import the dataset, shrec 16, a benchmark dataset for 3D mesh classification. We then lift each graph into our domain of choice, a hypergraph.\n",
    "\n",
    "We will also retrieve:\n",
    "- input signal on the edges for each of these hypergraphs, as that will be what we feed the model in input\n",
    "- the label associated to the hypergraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:53.022151550Z",
     "start_time": "2023-06-01T16:14:52.949636599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "\n",
      "done!\n",
      "(252, 6)\n",
      "(252, 6)\n",
      "(252, 6)\n",
      "(750, 10)\n",
      "(750, 10)\n",
      "(750, 10)\n"
     ]
    }
   ],
   "source": [
    "shrec, _ = datasets.mesh.shrec_16(size=\"small\")\n",
    "\n",
    "# Convert NpzFile objects to NumPy arrays\n",
    "shrec = {key: np.array(value) for key, value in shrec_training.items()}\n",
    "x_0s = shrec[\"node_feat\"]\n",
    "x_1s = shrec[\"edge_feat\"]\n",
    "print(x_0s[0].shape)\n",
    "print(x_0s[1].shape)\n",
    "print(x_0s[2].shape)\n",
    "print(x_1s[0].shape)\n",
    "print(x_1s[1].shape)\n",
    "print(x_1s[2].shape)\n",
    "\n",
    "ys = shrec[\"label\"]\n",
    "simplexes = shrec[\"complexes\"]\n",
    "\n",
    "hg_list = []\n",
    "hg_list_test = []\n",
    "for simplex in simplexes:\n",
    "    hg = simplex.to_hypergraph()\n",
    "    hg_list.append(hg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neighborhood structures. ##\n",
    "\n",
    "Now we retrieve the neighborhood structures (i.e. their representative matrices) that we will use to send messges on each simplicial complex. In the case of this architecture, we need the boundary matrix (or incidence matrix) $B_1$ with shape $n_\\text{nodes} \\times n_\\text{edges}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:54.320559691Z",
     "start_time": "2023-06-01T16:14:54.310717495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(250, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(250, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(250, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n",
      "(252, 1250)\n"
     ]
    }
   ],
   "source": [
    "incidence_1_list = []\n",
    "for hg in hg_list:\n",
    "    incidence_1 = hg.incidence_matrix()\n",
    "    print(incidence_1.shape)\n",
    "    incidence_1 = torch.from_numpy(incidence_1.todense()).to_sparse()\n",
    "    incidence_1_list.append(incidence_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Neural Network\n",
    "\n",
    "Using the TemplateLayer class, we create a neural network with stacked layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:55.343005145Z",
     "start_time": "2023-06-01T16:14:55.339481459Z"
    }
   },
   "outputs": [],
   "source": [
    "channels_edge = x_1s[0].shape[1]\n",
    "channels_node = x_0s[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:56.033274119Z",
     "start_time": "2023-06-01T16:14:56.029056913Z"
    }
   },
   "outputs": [],
   "source": [
    "class TemplateNN(torch.nn.Module):\n",
    "    \"\"\"Neural network implementation of Template for hypergraph classification.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    channels_edge : int\n",
    "        Dimension of edge features\n",
    "    channels_node : int\n",
    "        Dimension of node features\n",
    "    n_layer : 2\n",
    "        Amount of message passing layers.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels_edge, channels_node, n_layers=2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(\n",
    "                TemplateLayer(\n",
    "                    in_channels=channels_edge,\n",
    "                    intermediate_channels=channels_node,\n",
    "                    out_channels=channels_edge,\n",
    "                )\n",
    "            )\n",
    "        self.layers = torch.nn.ModuleList(layers)\n",
    "        self.linear = torch.nn.Linear(channels_edge, 1)\n",
    "\n",
    "    def forward(self, x_1, incidence_1):\n",
    "        \"\"\"Forward computation through layers, then linear layer, then global max pooling.\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        x_1 : tensor\n",
    "            shape = [n_edges, channels_edge]\n",
    "            Edge features.\n",
    "\n",
    "        incidence_1 : tensor\n",
    "            shape = [n_nodes, n_edges]\n",
    "            Boundary matrix of rank 1.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        _ : tensor\n",
    "            shape = [1]\n",
    "            Label assigned to whole complex.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x_1 = layer(x_1, incidence_1)\n",
    "        pooled_x = torch.max(x_1, dim=0)[0]\n",
    "        return torch.sigmoid(self.linear(pooled_x))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We specify the model, the loss, and an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:58.153514385Z",
     "start_time": "2023-06-01T16:14:57.243596119Z"
    }
   },
   "outputs": [],
   "source": [
    "model = TemplateNN(channels_edge, channels_node, n_layers=2)\n",
    "model = model.to(device)\n",
    "crit = torch.nn.BCELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:59.046068930Z",
     "start_time": "2023-06-01T16:14:59.037648626Z"
    }
   },
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "x_1_train, x_1_test = train_test_split(x_1s, test_size=test_size, shuffle=False)\n",
    "incidence_1_train, incidence_1_test = train_test_split(\n",
    "    incidence_1_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "y_train, y_test = train_test_split(ys, test_size=test_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell performs the training, looping over the network for a low amount of epochs. We keep training minimal for the purpose of rapid testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:15:01.683216142Z",
     "start_time": "2023-06-01T16:15:00.727075750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([252, 1250])\n",
      "torch.Size([750, 10])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of input face features does not have the correct number of edges 1250.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m x_1, incidence_1, y \u001b[38;5;241m=\u001b[39m x_1\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device), incidence_1\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device), torch\u001b[38;5;241m.\u001b[39mtensor(y, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 15\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincidence_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m crit(y_hat, y)\n\u001b[1;32m     17\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ((y_hat \u001b[38;5;241m>\u001b[39m threshold_probability_positive_class) \u001b[38;5;241m==\u001b[39m y\u001b[38;5;241m.\u001b[39mbool())\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tmx4/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[34], line 49\u001b[0m, in \u001b[0;36mTemplateNN.forward\u001b[0;34m(self, x_1, incidence_1)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward computation through layers, then linear layer, then global max pooling.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    Label assigned to whole complex.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 49\u001b[0m     x_1 \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincidence_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m pooled_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(x_1, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(pooled_x))[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tmx4/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/TopoModelX/topomodelx/nn/hypergraph/template_layer.py:66\u001b[0m, in \u001b[0;36mTemplateLayer.forward\u001b[0;34m(self, x_1, incidence_1)\u001b[0m\n\u001b[1;32m     64\u001b[0m incidence_1_transpose \u001b[38;5;241m=\u001b[39m incidence_1\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m!=\u001b[39m incidence_1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of input face features does not have the correct number of edges \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mincidence_1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m     )\n\u001b[1;32m     69\u001b[0m x_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_level1_1_to_0(x_1, incidence_1)\n\u001b[1;32m     70\u001b[0m x_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_level2_0_to_1(x_0, incidence_1_transpose)\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of input face features does not have the correct number of edges 1250."
     ]
    }
   ],
   "source": [
    "test_interval = 2\n",
    "num_epochs = 5\n",
    "threshold_probability_positive_class = 0.5\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    num_samples = 0\n",
    "    correct = 0\n",
    "    model.train()\n",
    "    for x_1, incidence_1, y in zip(x_1_train, incidence_1_train, y_train):\n",
    "        x_1 = torch.tensor(x_1)\n",
    "        print(incidence_1.shape)\n",
    "        print(x_1.shape)\n",
    "        x_1, incidence_1, y = (\n",
    "            x_1.float().to(device),\n",
    "            incidence_1.float().to(device),\n",
    "            torch.tensor(y, dtype=torch.float).to(device),\n",
    "        )\n",
    "        opt.zero_grad()\n",
    "        y_hat = model(x_1, incidence_1)\n",
    "        loss = crit(y_hat, y)\n",
    "        correct += (\n",
    "            ((y_hat > threshold_probability_positive_class) == y.bool()).sum().item()\n",
    "        )\n",
    "        num_samples += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "    train_acc = correct / num_samples\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f} Train_acc: {train_acc:.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            num_samples = 0\n",
    "            correct = 0\n",
    "            for x_1, incidence_1, y in zip(x_1_test, incidence_1_test, y_test):\n",
    "                x_1 = torch.tensor(x_1)\n",
    "                x_1, incidence_1, y = (\n",
    "                    x_1.float().to(device),\n",
    "                    incidence_1.float().to(device),\n",
    "                    torch.tensor(y, dtype=torch.float).to(device),\n",
    "                )\n",
    "                y_hat = model(x_1, incidence_1)\n",
    "                correct += (\n",
    "                    ((y_hat > threshold_probability_positive_class) == y.bool())\n",
    "                    .sum()\n",
    "                    .item()\n",
    "                )\n",
    "                num_samples += 1\n",
    "            test_acc = correct / num_samples\n",
    "            print(f\"Test_acc: {test_acc:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
