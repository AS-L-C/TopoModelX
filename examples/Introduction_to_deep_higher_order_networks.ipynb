{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%reload_ext nb_black"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Utilities to install torch_topo for colab.research.google\n",
    "\n",
    "pub_key = '''ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDIv3soblRf5t/hIHqflxr6ubw+8WxjJjaVVuUQyG7Vkn8JAamMBfETBLFdTonAq4gyfYZwr/YoLOGe0lhHW6zEEKQzLo6s9qPk7UlcjBx6jQ4Wz1fjmT36wEkcqgx8RVWnhi5csO5n/TZMVHII1U7BRu2uQM+R5N1Ck/IqyNbO7TMXm8R8wOEOfJUeEd4CSOMoq75TAvTubWkpO+wwl+xjph6MVTLgHJM6Hhu7HLAzV5xjHx7OrLeDszvd4J5OTPNJ8XqM1ehBkMKKkJhKgoIPFNmo93oHC1/XIaBNezYydoFReM0BSbT842Sd+KEmcRRryq6D+wKwgtbj15eEpUEBdgaGLMbPqL4yPRiS4QSinMJ1VANgoVTreCEl9FXN7FVlIX575C+fqlHzXm2wv9JJYxZKCIJLlj4V9efIVqfKpU8i+LlB4YChroQZHmQajs/vb1486a7rZacL3dzrPfNO9Nw4BmNYcvh7CSMP9S/5yI6GNECYqmUKUjCIj0AFUeYzigX7+VAl2x02rJ0dI9rSz41I2LTn4ln6NjrXZ1fhfNhOjGivpOxNKE2bNLzZHnFJXcDQ1/L9nj2whLYa4E4TYpwIHOdnz2s7bbzZnx7q3gQS13HBrZja4Xk7eFbukmy4s129E70moXy1YfrPiBXook/hTwZllwnff/hOmwxo9Q== repo-first@servername-deploy-user'''\n",
    "\n",
    "key = '''-----BEGIN OPENSSH PRIVATE KEY-----\n",
    "b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAACFwAAAAdzc2gtcn\n",
    "NhAAAAAwEAAQAAAgEAyL97KG5UX+bf4SB6n5ca+rm8PvFsYyY2lVblEMhu1ZJ/CQGpjAXx\n",
    "EwSxXU6JwKuIMn2GcK/2KCzhntJYR1usxBCkMy6OrPaj5O1JXIwceo0OFs9X45k9+sBJHK\n",
    "oMfEVVp4YuXLDuZ/02TFRyCNVOwUbtrkDPkeTdQpPyKsjWzu0zF5vEfMDhDnyVHhHeAkjj\n",
    "KKu+UwL07m1pKTvsMJfsY6YejFUy4ByTOh4buxywM1ecYx8ezqy3g7M73eCeTkzzSfF6jN\n",
    "XoQZDCipCYSoKCDxTZqPd6Bwtf1yGgTXs2MnaBUXjNAUm0/ONknfihJnEUa8qug/sCsILW\n",
    "49eXhKVBAXYGhizGz6i+Mj0YkuEEopzCdVQDYKFU63ghJfRVzexVZSF+e+Qvn6pR815tsL\n",
    "/SSWMWSgiCS5Y+FfXnyFanyqVPIvi5QeGAoa6EGR5kGo7P729ePOmu62WnC93c6z3zTvTc\n",
    "OAZjWHL4ewkjD/Uv+ciOhjRAmKplClIwiI9ABVHmM4oF+/lQJdsdNqydHSPa0s+NSNi05+\n",
    "JZ+jY612dX4XzYToxor6TsTShNmzS82R5xSV3A0Nfy/Z49sIS2GuBOE2KcCBznZ89rO228\n",
    "2Z8e6t4EEtdxwa2Y2uF5O3hW7pJsuLNdvRO9JqF8tWH6z4gV6KJP4U8GZZcJ33/4TpsMaP\n",
    "UAAAdY4vwqy+L8KssAAAAHc3NoLXJzYQAAAgEAyL97KG5UX+bf4SB6n5ca+rm8PvFsYyY2\n",
    "lVblEMhu1ZJ/CQGpjAXxEwSxXU6JwKuIMn2GcK/2KCzhntJYR1usxBCkMy6OrPaj5O1JXI\n",
    "wceo0OFs9X45k9+sBJHKoMfEVVp4YuXLDuZ/02TFRyCNVOwUbtrkDPkeTdQpPyKsjWzu0z\n",
    "F5vEfMDhDnyVHhHeAkjjKKu+UwL07m1pKTvsMJfsY6YejFUy4ByTOh4buxywM1ecYx8ezq\n",
    "y3g7M73eCeTkzzSfF6jNXoQZDCipCYSoKCDxTZqPd6Bwtf1yGgTXs2MnaBUXjNAUm0/ONk\n",
    "nfihJnEUa8qug/sCsILW49eXhKVBAXYGhizGz6i+Mj0YkuEEopzCdVQDYKFU63ghJfRVze\n",
    "xVZSF+e+Qvn6pR815tsL/SSWMWSgiCS5Y+FfXnyFanyqVPIvi5QeGAoa6EGR5kGo7P729e\n",
    "POmu62WnC93c6z3zTvTcOAZjWHL4ewkjD/Uv+ciOhjRAmKplClIwiI9ABVHmM4oF+/lQJd\n",
    "sdNqydHSPa0s+NSNi05+JZ+jY612dX4XzYToxor6TsTShNmzS82R5xSV3A0Nfy/Z49sIS2\n",
    "GuBOE2KcCBznZ89rO2282Z8e6t4EEtdxwa2Y2uF5O3hW7pJsuLNdvRO9JqF8tWH6z4gV6K\n",
    "JP4U8GZZcJ33/4TpsMaPUAAAADAQABAAACAQCbEU5RLOiALBdED5KpNE/i3RZMiLDw5dPE\n",
    "CBtpm2oCD+eRTXdqciyGRFm9ea1u5xGN7uiqo0HCRukBhforJOWPjktIPe54LQztCV5h3t\n",
    "kg0VTQydlaNkVqLJ5NRg3e8K/5d+zYLNP7/9yxmYl8yUjMg72VnPq598/yXdXytrE3poKQ\n",
    "+gINW/B9AD8vux2DJkpSYFfkR6RriJhzAZaw8qnnT6rJPAbQl0Ii4p8JKl/xSPvElYV2oH\n",
    "Qhh5H3Eks2sLo9pArGP3XpL6kW4BrDc4yDmdeIbaIVTvP5ekAZmdODp4a9KFMza3wi3IQc\n",
    "grH4v+DGYwnraTnqnybfWOnxzLUoywT+M3zN/ZLOO7qyKtEdRJ7qPj7gDdBOClY+rvxL0Q\n",
    "X0hC9vre2LrFMC6kNo5v3yzTh7pyDo4QcPOXvrcHJ3u9BoGZc9Wvdm2Rpsuhh1oXA0gPP8\n",
    "PdMZUblPK/WePEybAplp5SRHtFqiRa1/8E8w3iT1zBi/si7Cnw7zLEMp8RF02EK9CiaD54\n",
    "01tGlQZxgd5fINyOOUB22cB6hSkjEHQAB5VPIpw7F7hGOhagGvyvrm8Z/B2ZFagMtaiJss\n",
    "uIabXzWw8+wtphDnlmJD3TSDLzYxHhmLMNhThUrY1DpKmw2oJT/D+Ji+MNbdg8ugGdDHUP\n",
    "d+TbjXJ2fgdHJLJc6QAQAAAQEAyipmWSSC4yC7c3IVfAJybhLeYrdY1prt6fSqoPV+5MRN\n",
    "pXVW7BoxrUH+l+qy86UgZqdbOi6LFYgniOKY8/AVyqXnvN3uCxC22+qEQXTmWw/81uQrJ8\n",
    "G2O7AYMvDQZ2Rc3bjFDCxaRt+7SEb6gxA4uThgIXAAiUg5VqC2NboVoIsbOC8zOd/Ns6vS\n",
    "3v9aTibusBUvKUn7/ODL/Tpm9FRNdZzIMMrwmHBisHFJkKnuCNQpkTNkU6ajgmgFCOFaXX\n",
    "+NI3ikf08bzx7KbWh46Hhr1qLvOxjRnyDcLsRmdNqFAmE5cRZqn+xLazCBM6dQKYYsycrT\n",
    "V1hP866fuHetYMY4iAAAAQEA/4ZEOJl/9JE9XM1IN0PlPQhAhYLZyntjc7MYCKCBbIpz05\n",
    "e35qRxB9Z+rsQMwrfZr3gOFGVnTc6AcS3Qn2aYfPMNjhvDQsMjTL6DjnU94l1EGvVIJGCD\n",
    "3kmKaoeC2fa9JEnspL3k8crtZLNAEL1GhDjBwFfNCQg6v7k13PiOxXJaM8ImpA6gl7gZbl\n",
    "dO3LSoUzovlu7ONPqayYHi9nct6fX2Ea7ijGdMiR0f1n0BqaW0iLluXnUj/eKTyK7yU1Wa\n",
    "+RKPqOPWCqXlO7oXxRUWuM0MtuIfdQjdVjIBC/Qg29VKaVmHzBKHVinOhb024RhG0/1sxR\n",
    "o/e+dzsZei6Zpr8QAAAQEAyR8eZC6ZMuCEf83PfWXBeKrCFx0EBi4vNHgKqGqQZXzqx7t1\n",
    "hVM5gfaGLTQP3s7lxiue0+2PJ9RQ52FtS/pwYuXF6XxeAIlg2N2BEa/VlZoK4P0yqGy1qe\n",
    "aQJkhpG+vyLs880ywuxKUvn4Cc7hStNk7evkXxw8HX6rsyCGqgrnVGhEO5wiSlO4qCNgZZ\n",
    "CXPQhfQCoKy1x0FrIUrIxvKyWtrvguul/IxzPfCIkYfiEoQgvlcQTbP1XiRHWqoA94M2Ms\n",
    "Fg8MrDMYWCqLy30k8/adw9sNncv1dUxv+NHkqyvjx4tjv9VCSudPeP/V4aPjUheTXlyWaf\n",
    "TuwCesiPvPdhRQAAACFyZXBvLWZpcnN0QHNlcnZlcm5hbWUtZGVwbG95LXVzZXI=\n",
    "-----END OPENSSH PRIVATE KEY-----\n",
    "'''\n",
    "\n",
    "config ='''Host torch-topo-clone github.com\n",
    "    HostName github.com\n",
    "    IdentityFile ~/.ssh/id_rsa\n",
    "'''\n",
    "\n",
    "known_hosts ='''github.com,140.82.121.3 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEmKSENjQEezOmxkZMy7opKgwFB9nkt5YRrYMjNuG5N87uRgg6CLrbo5wAdT/y6v0mKV0U2w0WZ2YB/++Tpockg=\n",
    "140.82.121.4 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEmKSENjQEezOmxkZMy7opKgwFB9nkt5YRrYMjNuG5N87uRgg6CLrbo5wAdT/y6v0mKV0U2w0WZ2YB/++Tpockg=\n",
    "'''\n",
    "\n",
    "def install_torch_topo_for_google_colab(device = 'cpu'):\n",
    "\n",
    "  def nice_printer(res):\n",
    "    if res:\n",
    "      print(res)\n",
    "\n",
    "  try:\n",
    "    import google.colab\n",
    "    in_colab = True\n",
    "\n",
    "  except:\n",
    "    print(\"Not running in Google Colab, exiting...\")\n",
    "    in_colab = False\n",
    "\n",
    "  if in_colab:\n",
    "    #################### Clone Repo\n",
    "\n",
    "    print(\"Runs in Colab, cloning torch_topo...\")\n",
    "\n",
    "    out = ! mkdir -p /root/.ssh\n",
    "    nice_printer(out)\n",
    "\n",
    "    with open(r'/root/.ssh/id_rsa', 'w', encoding='utf8') as fh:\n",
    "        fh.write(key)\n",
    "    out = ! chmod 600 /root/.ssh/id_rsa\n",
    "    nice_printer(out)\n",
    "\n",
    "    with open(r'/root/.ssh/id_rsa.pub', 'w', encoding='utf8') as fh:\n",
    "        fh.write(pub_key)\n",
    "    out = ! chmod 600 /root/.ssh/id_rsa.pub\n",
    "    nice_printer(out)\n",
    "\n",
    "    with open(r'/root/.ssh/config', 'w', encoding='utf8') as fh:\n",
    "        fh.write(config)\n",
    "    out = ! chmod 600 ~/.ssh/config\n",
    "    nice_printer(out)\n",
    "\n",
    "    with open(r'/root/.ssh/known_hosts', 'w', encoding='utf8') as fh:\n",
    "        fh.write(known_hosts)\n",
    "    out = ! chmod 600 ~/.ssh/known_hosts\n",
    "    nice_printer(out)\n",
    "\n",
    "    out = ! git clone git@torch-topo-clone:pyt-team/torch_topo.git pyt_torch_topo\n",
    "    nice_printer(out)\n",
    "\n",
    "    import sys\n",
    "    sys.path.append('/content/pyt_torch_topo')\n",
    "\n",
    "    #################### Install package\n",
    "    if device == 'cpu':\n",
    "      out = ! pip install torch==1.12.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "      nice_printer(out)\n",
    "      out = ! pip install torch-scatter -f https://data.pyg.org/whl/torch-1.12.0+cpu.html\n",
    "      nice_printer(out)\n",
    "      out = ! pip install torch-sparse -f https://data.pyg.org/whl/torch-1.12.0+cpu.html\n",
    "      nice_printer(out)\n",
    "      out = get_ipython().run_line_magic('cd', 'pyt_torch_topo')\n",
    "      out = ! pip install -e .[full]\n",
    "      nice_printer(out)\n",
    "      out = get_ipython().run_line_magic('cd', '..')\n",
    "    elif device == 'cuda':\n",
    "      out = ! pip install torch==1.12.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "      nice_printer(out)\n",
    "      out = ! pip install torch-scatter -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
    "      nice_printer(out)\n",
    "      out = ! pip install torch-sparse -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
    "      nice_printer(out)\n",
    "      out = get_ipython().run_line_magic('cd', 'pyt_torch_topo')\n",
    "      out = ! pip install -e .[full]\n",
    "      nice_printer(out)\n",
    "      out = get_ipython().run_line_magic('cd', '..')\n",
    "    else:\n",
    "      raise Exception(\"Unsupported device, device must be either 'cpu' or 'cuda'!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "install_torch_topo_for_google_colab(\n",
    "    device=\"cpu\"\n",
    ")  # for 'cuda' enable in colab Edit > Notebook Settings > Hardware accelerator > GPU"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from torch_topo.nn import LTN, MergeOper, SplitOper, MultiMergeOper, MultiSplitOper\n",
    "from torch_topo.nn import (\n",
    "    BatchLTN,\n",
    "    BatchMergeOper,\n",
    "    BatchSplitOper,\n",
    "    BatchMultiMergeOper,\n",
    "    BatchMultiSplitOper,\n",
    ")\n",
    "from torch_topo.topology import SimplicialComplex\n",
    "from torch_topo.util import coo_2_torch_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from torch_topo.nn import LTN, MergeOper, SplitOper, MultiMergeOper, MultiSplitOper\n",
    "from torch_topo.nn import (\n",
    "    BatchLTN,\n",
    "    BatchMergeOper,\n",
    "    BatchSplitOper,\n",
    "    BatchMultiMergeOper,\n",
    "    BatchMultiSplitOper,\n",
    ")\n",
    "from torch_topo.topology import SimplicialComplex\n",
    "from torch_topo.util import coo_2_torch_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction:\n",
    "\n",
    "Given a complex $\\mathcal{X}$ (simplicial, polyhedral or cellular), the general form conv operator on $\\mathcal{X}$ has the form \n",
    "\n",
    "\\begin{equation}\n",
    "X^{i+1}_{t}= M(G_1,...,G_n,X^{i}_{s_1},...,X^{i}_{s_k}, W^i)\n",
    "\\label{eq:vector_ray} \\tag{1}\n",
    "\\end{equation}\n",
    " where $G_i :C^{j_i}(\\mathcal{X})\\to C^{k_i}(\\mathcal{X}) $ are cochain operators, $X^{i}_{j}$ is a $j$-cochain ( $i$ indicates the iteration stage), $W^i$ is a trainable parameter, $M$ is a message passing function that takes into account all the messages induced by the operators $G_1,...,G_n$ which collectively act on the cochains $X^{i}_{s_1},...,X^{i}_{s_k}$ to obtain the final updated cochain $X^{i+1}_{t}$.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It is key to understand that on higher order networks in general, the operator $A_i$ usually reflects the _structure_ of the domain $\\mathcal{X}$, although it is also common to also break this rule in practional scenarios. For instance, when $\\mathcal{X}$ is a simplicial/cell complex then this structure can be encoded in practice by the boundary maps. It is thus desirable to use these maps as the means to perform the message passing. On such complexes, other operators can also be utilized such as the $k$-Hodge Laplacian operators or higher order adjacency operators. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The simplest case implementation in our package:\n",
    "\n",
    "In Higher Order Deep Networks, the simplest type of the operator (1) has the form :\n",
    "    \\begin{equation}\n",
    "X^{i+1}_{t}= M(G,X^{i}_{s}, W^i)=G*X^i_s*W^i     \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In our library is implemented in the function LTN. To explain how LTN works in our library, we create a simple network that operates on a simple simplicial complex. The simplicial complex that we will use for the example is determined by the following simplices:  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "simplices = [(0, 1, 2), (1, 2, 3), (2, 3), (1, 2, 4), (5, 3), (0, 4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Recall here that a simplicial complex is determined completely by its highest dimensional simplices so the above list contains only the highest dimensional simplices in the complex. In other words, the above complex, has 3 faces, x edges and y vertices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Creating a simple simplicial complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Class for construction boundary operators, Hodge Laplacians,\\n    higher order (co)adjacency operators from collection of\\n    simplices.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HL = SimplicialComplex(\n",
    "    simplices, mode=\"gudhi\"\n",
    ")  # other available modes : gudhi--typically much faster\n",
    "\n",
    "HL.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can check the nodes, edges, faces of the above simplex as follows : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(frozenset({0}), frozenset({1}), frozenset({2}), frozenset({3}), frozenset({4}), frozenset({5}))\n",
      "(frozenset({0, 1}), frozenset({0, 2}), frozenset({0, 4}), frozenset({1, 2}), frozenset({1, 3}), frozenset({1, 4}), frozenset({2, 3}), frozenset({2, 4}), frozenset({3, 5}))\n",
      "(frozenset({0, 1, 2}), frozenset({1, 2, 3}), frozenset({1, 2, 4}))\n"
     ]
    }
   ],
   "source": [
    "print(HL.n_faces(0))\n",
    "print(HL.n_faces(1))\n",
    "print(HL.n_faces(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "We can count the number of i-simplices by simply measuring the length of the above lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of nodes  6\n",
      "num of edges  9\n",
      "num of faces  3\n"
     ]
    }
   ],
   "source": [
    "N0 = len(HL.n_faces(0))\n",
    "N1 = len(HL.n_faces(1))\n",
    "N2 = len(HL.n_faces(2))\n",
    "\n",
    "print(\"num of nodes \", N0)\n",
    "print(\"num of edges \", N1)\n",
    "print(\"num of faces \", N2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Computing structure operators of the complex:\n",
    "\n",
    "The boundary matrices and the k-Hodge Laplacians can be computed as follow :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first-boundary matrix is: \n",
      " [[-1. -1. -1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0. -1. -1. -1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0. -1. -1.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  1.  0. -1.]\n",
      " [ 0.  0.  1.  0.  0.  1.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.]] \n",
      " and it has shape (6, 9)\n",
      "The 2nd-boundary matrix is: \n",
      " [[ 1.  0.  0.]\n",
      " [-1.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 1.  1.  1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 0.  0. -1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  0.]] \n",
      " and it has shape (9, 3)\n"
     ]
    }
   ],
   "source": [
    "B1 = HL.get_boundary_operator(1)  # B1: E(X)->V(X)\n",
    "print(\"The first-boundary matrix is: \\n\", B1.toarray(), \"\\n and it has shape\", B1.shape)\n",
    "\n",
    "B2 = HL.get_boundary_operator(2)  # B2: F(X)->E(X)\n",
    "print(\"The 2nd-boundary matrix is: \\n\", B2.toarray(), \"\\n and it has shape\", B2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "and the k-Hodge Laplacians are given as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1-Hodge Laplacian: \n",
      " [[ 3.  0.  1.  0. -1. -1.  0.  0.  0.]\n",
      " [ 0.  3.  1.  0.  0.  0. -1. -1.  0.]\n",
      " [ 1.  1.  2.  0.  0.  1.  0.  1.  0.]\n",
      " [ 0.  0.  0.  5.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  3.  1.  0.  0. -1.]\n",
      " [-1.  0.  1.  0.  1.  3.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  3.  1. -1.]\n",
      " [ 0. -1.  1.  0.  0.  0.  1.  3.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0. -1.  0.  2.]] \n",
      " and it has shape (9, 9)\n"
     ]
    }
   ],
   "source": [
    "L1 = HL.get_hodge_laplacian(1)  # L1: E(X)->E(X)\n",
    "print(\"The 1-Hodge Laplacian: \\n\", L1.toarray(), \"\\n and it has shape\", L1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 0-Hodge Laplacian: \n",
      " [[ 3. -1. -1.  0. -1.  0.]\n",
      " [-1.  4. -1. -1. -1.  0.]\n",
      " [-1. -1.  4. -1. -1.  0.]\n",
      " [ 0. -1. -1.  3.  0. -1.]\n",
      " [-1. -1. -1.  0.  3.  0.]\n",
      " [ 0.  0.  0. -1.  0.  1.]] \n",
      " and it has shape (6, 6)\n"
     ]
    }
   ],
   "source": [
    "L0 = HL.get_hodge_laplacian(0)  # L0: V(X)->V(X), L0=D-A\n",
    "print(\"The 0-Hodge Laplacian: \\n\", L0.toarray(), \"\\n and it has shape\", L0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2-Hodge Laplacian: \n",
      " [[3. 1. 1.]\n",
      " [1. 3. 1.]\n",
      " [1. 1. 3.]] \n",
      " and it has shape (3, 3)\n"
     ]
    }
   ],
   "source": [
    "L2 = HL.get_hodge_laplacian(2)  # L2: F(X)->F(X)\n",
    "print(\"The 2-Hodge Laplacian: \\n\", L2.toarray(), \"\\n and it has shape\", L2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's create features that live on the top of the above simplex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "feature_input_space_dim = 3\n",
    "batch_size = 10\n",
    "x_v = torch.rand(batch_size, N0, feature_input_space_dim)\n",
    "x_e = torch.rand(batch_size, N1, feature_input_space_dim)\n",
    "x_f = torch.rand(batch_size, N2, feature_input_space_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We also convert the operator to Torch tensors :\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "L0 = coo_2_torch_tensor(L0)\n",
    "L1 = coo_2_torch_tensor(L1)\n",
    "L2 = coo_2_torch_tensor(L2)\n",
    "B1 = coo_2_torch_tensor(B1)\n",
    "B2 = coo_2_torch_tensor(B2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Creating a simple higher order network\n",
    "Now we create a simple network :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LTN (3 -> 10)\n"
     ]
    }
   ],
   "source": [
    "model = LTN(feature_input_space_dim, 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L1.matmul(x_e[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we pass the operator as well as the signal to the network : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_out_e = model(x_e[0], L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here recall that $L_1: C^1(\\mathcal{X},d)\\to C^1(\\mathcal{X},d) $, where $C^1(\\mathcal{X},d) = \\{ f: E(\\mathcal{X}) \\to \\mathbb{R}^d  \\} $, here $E(\\mathcal{X})$ denote the edges of the complex $\\mathcal{X}$. The network defined above specifies a mapping  $x_{out} \\to model(x_{in},L_1)$ where the mapping is viewed as : $model_{L_1}: C^1(\\mathcal{X},d_{in})\\to C^1(\\mathcal{X},d_{out}) $, in our case $d_{in}=3$ and $d_{out}=5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "One way to understand the above network inputs is as follows: as long as the operator $L_1$ meaningfully act by right multiplication on the signal $x$ moving it between two different cochain spaces, then the network should admit similar computation between the same cochain spaces induced by the same operator. Consider for instance the shape of the output tensor :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 10])\n"
     ]
    }
   ],
   "source": [
    "print(x_out_e.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Observe that the above vector lives on edges, (thinking of L1 as a map that maps edge signal to edge signal),\n",
    "it has dimension 5, that is determined by the constructor of the network above, and it has batch size =10. Now if we to act on the same tensor x_e, by a different operator, say B1, then we can also write :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_v_out = model(x_e[0], B1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 10])\n"
     ]
    }
   ],
   "source": [
    "print(x_v_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Observe that the above tensor lives on the vertices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Similarly, we can act on the nodes, or faces :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 10])\n"
     ]
    }
   ],
   "source": [
    "x_v_out_ = model(x_v[0], L0)\n",
    "print(x_v_out_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 10])\n"
     ]
    }
   ],
   "source": [
    "x_e_out_ = model(x_f[0], B2)\n",
    "print(x_e_out_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Working with normalized operators\n",
    "\n",
    "When training a higher deep learning model, one usually need to normalize the structure operator. In our package, this can be done calling the appropriate normalized operator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we first take a look at the higher order adj matrix between the edges :\n",
    "Adj0 = HL.get_higher_order_adj(0)\n",
    "\n",
    "\n",
    "Adj0.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 9)\n"
     ]
    }
   ],
   "source": [
    "# we can get the normalized version by using the command :\n",
    "Adj1_norm = HL.get_normalized_higher_order_coadj(1)\n",
    "print(Adj1_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 9)\n",
      "(6, 9)\n"
     ]
    }
   ],
   "source": [
    "# we can get the normalized Laplacian or the boundary operator using similar commands :\n",
    "\n",
    "L1_norm = HL.get_normalized_hodge_laplacian(1)\n",
    "\n",
    "print(L1_norm.shape)\n",
    "\n",
    "\n",
    "B1_norm = HL.get_normalized_boundary_operator(1)\n",
    "\n",
    "print(B1_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Batch higher order network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Batch higher order conv operator is also supported in our package. It is implemented with the class BatchLTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchLTN (3 -> 5)\n"
     ]
    }
   ],
   "source": [
    "batchmodel = BatchLTN(feature_input_space_dim, 5)\n",
    "print(batchmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 9, 5])\n"
     ]
    }
   ],
   "source": [
    "x_out_e = batchmodel(x_v, B1.t())\n",
    "print(\n",
    "    x_out_e.shape\n",
    ")  # output tensor size = [batch_num, number of target cells, target feature dimension]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Making more complicated network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n",
    "Now in order to make more complex models such as the above in Definition (1) above, we use the split and the merge operators.\n",
    "\n",
    "### The merge operator\n",
    "\n",
    "Let's assume that we want to merge cochains that live on the nodes and the faces together to a cochain that live on the edges. We assume that cochain live on the nodes, and the cochain live on the faces have dimensions 3 and 5 respectively, we also assume that desired output cochain on the edges have dim 10. We create a merge operator with these dimension. This can be done easily using the following :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "merge_model = MergeOper(in_ch_1=3, in_ch_2=5, target_ch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we create some input features to run them in the merge model above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "feature_input_space_dim_v = 3\n",
    "feature_input_space_dim_f = 5\n",
    "\n",
    "x_v = torch.rand(batch_size, N0, feature_input_space_dim_v)\n",
    "x_f = torch.rand(batch_size, N2, feature_input_space_dim_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_edges_out = merge_model(x_v[0], x_f[0], B1.t(), B2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Observe that the features \"x_edges_out\" live on the edges, one can see that by inspecting the second channel in the shape of the output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 5])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B1.t().matmul(x_v[0]).shape\n",
    "B2.matmul(x_f[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the model above, should think as the operator B1.t() as acting on the feature x_v and the operator B2 as acting on the feature x_f. Here the operator $B1.t(): C^0(\\mathcal{X})\\to C^1(\\mathcal{X}) $, and the operator $B2 : C^2(\\mathcal{X})\\to C^1(\\mathcal{X})$. Observe that both of these operators have the same codomains. Hence the network merge can be thought about as a function $merge : C^0 (\\mathcal{X}, 3 ) \\times C^2 (\\mathcal{X}, 5 )\\to C^1(\\mathcal{X},10). $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The class MergeOper expects two input operators. One may choose these operators to be None (either one or both). In this case the merge operator acts as the identity. Note that in this case, the the channels for the input and the outputs must be chosen to be equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "merge_model_example2 = MergeOper(in_ch_1=3, in_ch_2=3, target_ch=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "xv_out = merge_model_example2(x_v[0], x_e[0], None, B1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that when both input operators are None then the merge operator acts according to the parameter merge_type (default is sum).\n",
    "\n",
    "One may in fact try both operators to be None. In this case, the merge operator acts like the identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xe_out = merge_model_example2(None, x_e[0], None, None)\n",
    "xe_out == x_e[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A batch version of the merge operator is also supported, this can be done using BatchMergeOper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "merge_model_example2 = BatchMergeOper(in_ch_1=3, in_ch_2=3, target_ch=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "xv_out = merge_model_example2(x_v, x_v, L0, None)\n",
    "\n",
    "print(xv_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember that None acts like the identity, so merge here with the default merge operation (sum) will add the the two input vectors:\n",
    "xv_out = merge_model_example2(x_v, x_v, None, None)\n",
    "(x_v + x_v)[0] == xv_out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Sharing parameters: \n",
    "Sometimes it is desirable to share parameters when merging signals from two different sources. In this case the transformation goes $(x,y) \\to  G_1 W x + G_2 W y$ where $+$ is the general merge operation and $W$ is the shared parameter. Here $G_1$ and $G_2$ are operators that act on the cochains x and y respectivly and have the same cochain codomain space. This the merge operation $(x,y) \\to  G_1 W x + G_2 W y$ is only possible however when the $x$ and $y$ are in the same dimension. One may still perform similar operation when $x$ and $y$ are from different dimension by transforming $x$ and $y$ via an affine map to the same ambient space then apply the above operations.   \n",
    "\n",
    "In our code this can be done as follows  :\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# first we define the input data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "feature_input_space_dim_v = 3\n",
    "feature_input_space_dim_e = 3\n",
    "feature_input_space_dim_f = 3\n",
    "\n",
    "x_v = torch.rand(N0, feature_input_space_dim_v)\n",
    "x_e = torch.rand(N1, feature_input_space_dim_e)\n",
    "\n",
    "x_f = torch.rand(N2, feature_input_space_dim_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# then we define the model :\n",
    "merge_model_example3 = MergeOper(\n",
    "    in_ch_1=3, in_ch_2=3, target_ch=3, shared_parameters=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "xv_out = merge_model_example3(x_v, x_e, L0, B1)\n",
    "\n",
    "print(xv_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initial_linear_shift is disabled\n",
    "\n",
    "merge_model_example4 = MergeOper(\n",
    "    in_ch_1=3,\n",
    "    in_ch_2=3,\n",
    "    target_ch=3,\n",
    "    shared_parameters=True,\n",
    "    initial_linear_shift=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the above code, because \"initial_linear_shift\" is false, the input (x,y) features will be only transformed by a single operation\n",
    "$(x,y) \\to  G_1 W x + G_2 W y$ with the same shared parameter $W$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in code this can be done as follows:\n",
    "\n",
    "xv_out = merge_model_example4(x_v, x_e, L0, B1)\n",
    "xv_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The split operator\n",
    "\n",
    "The split operator is the dual of the merge operator, it takes one signal and splits it apart into two signals by sending it to two different cochains.\n",
    "Given two linear cochain maps $G_1 : C^i(\\mathcal{X})\\to C^{j_1}(\\mathcal{X}) $ and $G_1 : C^i(\\mathcal{X})\\to C^{j_2}(\\mathcal{X}) $, the split_operator network \n",
    "        splits the signal from $C^i$ to two signals in $C^{j_1}(\\mathcal{X})$ and $C^{j_2}(\\mathcal{X})$ using the maps $G_1$ and $G_2$. Let's consider for example the following split model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "split_model = SplitOper(\n",
    "    num_features_in=3, target_ch_1=7, target_ch_2=3, shared_parameters=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "xv_out_1, xe_out_2 = split_model(x_v, L0, B1.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes output features shape  torch.Size([6, 7])\n",
      "edges output features shape  torch.Size([9, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"nodes output features shape \", xv_out_1.shape)\n",
    "print(\"edges output features shape \", xe_out_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As a map the above split operator is a function $split : C^0 (\\mathcal{X}, 3 ) \\to C^0(\\mathcal{X},7) \\times C^1(\\mathcal{X},12) . $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Shared parameters with split\n",
    "Sharing parameters is also possible with the split operation. This can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes output features shape  torch.Size([6, 3])\n",
      "edges output features shape  torch.Size([9, 3])\n"
     ]
    }
   ],
   "source": [
    "split_model = SplitOper(\n",
    "    num_features_in=3, target_ch_1=3, target_ch_2=3, shared_parameters=True\n",
    ")\n",
    "xv_out_1, xe_out_2 = split_model(x_v, L0, B1.t())\n",
    "print(\"nodes output features shape \", xv_out_1.shape)\n",
    "print(\"edges output features shape \", xe_out_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that when the shared_parameters is True in the split operation, the target channels must be equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## MultiMerge and MultSplit Operator:\n",
    "\n",
    "We now consider examples on how the multi_merge and multi_split operators work.\n",
    "\n",
    "### MultiSplit operator:\n",
    "\n",
    "The multi-split operator is similar to the split operator we defined above, the only difference is that it can split one singal into $n$ signal. We see an example next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "multi_split_model = MultiSplitOper(num_features_in=3, target_channels_list=[3, 3, 3])\n",
    "out_1, out_2, out_3 = multi_split_model(x_v, [L0, B1.t(), L0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node out features:  torch.Size([6, 3])\n",
      "edges out features:  torch.Size([9, 3])\n",
      "nodes out feature:  torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"node out features: \", out_1.shape)\n",
    "print(\"edges out features: \", out_2.shape)\n",
    "print(\"nodes out feature: \", out_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### MultiMerge operator:\n",
    "\n",
    "Now we try the multi merge operator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 9, 3])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_input_space_dim = 3\n",
    "batch_size = 10\n",
    "\n",
    "# define some random feature input\n",
    "x_v = torch.rand(batch_size, N0, feature_input_space_dim)\n",
    "x_e = torch.rand(batch_size, N1, feature_input_space_dim)\n",
    "x_f = torch.rand(batch_size, N2, feature_input_space_dim)\n",
    "\n",
    "\n",
    "# define the model\n",
    "multi_merge_model = BatchMultiMergeOper(\n",
    "    in_ch_list=[3, 3, 3],\n",
    "    target_ch=3,\n",
    "    merge_type=\"sum\",\n",
    "    shared_parameters=True,\n",
    "    initial_linear_shift=False,\n",
    ")\n",
    "# infer example 1\n",
    "out_sum = multi_merge_model([x_v, x_v, x_v], [None, None, None])\n",
    "\n",
    "assert (out_sum == x_v * 3).any()\n",
    "\n",
    "# infer example 2\n",
    "out_sum = multi_merge_model([x_v, x_e, x_f], [B1.t(), None, B2])\n",
    "out_sum.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Observe that the above merge map, merges features that is obtained from upper (nodes) and lower (faces) as well as same dimension (edges), all together to the final signal that lives on edges. One can see by inspecting the dimesion of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 9, 3])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_sum.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Batch and non-batch models are suppoted just as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 5])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_input_space_dim = 3\n",
    "batch_size = 10\n",
    "\n",
    "# define some random feature input\n",
    "x_v = torch.rand(N0, feature_input_space_dim)\n",
    "x_e = torch.rand(N1, feature_input_space_dim)\n",
    "x_f = torch.rand(N2, feature_input_space_dim)\n",
    "\n",
    "\n",
    "# define the model\n",
    "multi_merge_model = MultiMergeOper(\n",
    "    in_ch_list=[3, 3, 3],\n",
    "    target_ch=5,\n",
    "    merge_type=\"sum\",\n",
    "    shared_parameters=True,\n",
    "    initial_linear_shift=False,\n",
    ")\n",
    "# infer\n",
    "out_sum = multi_merge_model([x_v, x_e, x_f], [B1.t(), L1, B2])\n",
    "\n",
    "out_sum.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Multi-Split Operator\n",
    "\n",
    "The multi split operator supports splitting across n channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "feature_input_space_dim = 3\n",
    "\n",
    "# define some random feature input\n",
    "x_v = torch.rand(N0, feature_input_space_dim)\n",
    "x_e = torch.rand(N1, feature_input_space_dim)\n",
    "x_f = torch.rand(N2, feature_input_space_dim)\n",
    "\n",
    "# define the model\n",
    "multi_split_model = MultiSplitOper(\n",
    "    num_features_in=3, target_channels_list=[3, 3, 3], shared_parameters=False\n",
    ")\n",
    "# infer\n",
    "[ze_out1, zf_out1, ze_out] = multi_split_model(x_e, [B1, B2.t(), None])\n",
    "\n",
    "print(ze_out1.shape)\n",
    "print(zf_out1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Simplicial Tensor Network Example\n",
    "\n",
    "Now we see how we can use the above classes to build an arbitrary STN which facilitate general-purpose computation on a simplicial complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class STN_R(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    This example is similar to a model proposed in [1].\n",
    "\n",
    "    [1] Roddenberry, T. Mitchell, Nicholas Glaze, and Santiago Segarra. \"Principled simplicial neural networks for trajectory prediction.\" International Conference on Machine Learning. PMLR, 2021.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch_e, hidden_dim, target_ch_e, dropout=0.1, activation=None):\n",
    "        super(STN_R, self).__init__()\n",
    "\n",
    "        self.in_ch_in = in_ch_e\n",
    "        self.hidden = hidden_dim\n",
    "        self.in_ch_out = target_ch_e\n",
    "\n",
    "        self.multi_split = MultiSplitOper(\n",
    "            num_features_in=in_ch_e,\n",
    "            target_channels_list=[hidden_dim, hidden_dim, hidden_dim],\n",
    "        )\n",
    "\n",
    "        self.multi_merge = MultiMergeOper(\n",
    "            in_ch_list=[hidden_dim, hidden_dim, hidden_dim],\n",
    "            target_ch=target_ch_e,\n",
    "            merge_type=\"sum\",\n",
    "            shared_parameters=True,\n",
    "        )\n",
    "        self.act = activation\n",
    "\n",
    "    def forward(self, xe, Gv2v, Ge2e, Gf2f, Ge2v, Gf2e):\n",
    "\n",
    "        zv, ze, zf = self.multi_split(xe, [Ge2v, Ge2e, Gf2e.t()])\n",
    "\n",
    "        ze = self.multi_merge([zv, ze, zf], [Ge2v.t(), Ge2e, Gf2e])\n",
    "\n",
    "        if self.act != None:\n",
    "            ze = self.act(ze)\n",
    "\n",
    "        return ze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_e = torch.rand(N1, feature_input_space_dim)\n",
    "model = STN_R(3, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 3])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_e_out = model(x_e, L0, L1, L2, B1, B2)\n",
    "x_e_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}