{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from stnets.layers import LTN, MergeOper, SplitOper, MultiMergeOper, MultiSplitOper\n",
    "from stnets.layers.attention import HigherOrderAttentionLayer, SparseHigherOrderAttentionLayer\n",
    "from stnets.layers.attention import MultiHeadHigherOrderAttention, SpMultiHeadHigherOrderAttention\n",
    "\n",
    "from stnets.layers.attention import (\n",
    "    MultiHeadHigherOrderAttentionClassifer,\n",
    "    SpMultiHeadHigherOrderAttentionClassifer,\n",
    ")\n",
    "\n",
    "from stnets.topology import SimplicialComplex\n",
    "from stnets.modality.mesh_to_simplicial_complex import (coo_2_torch_tensor, \n",
    "                                                        mesh_2_operators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Simplicial Attention:\n",
    "\n",
    "Given a simplical complex $\\mathcal{X}$, the simplicial attention mechanism defines attention between two simplices $i$ and $j$ that are related via a simplicial structure map such as the boundary/k-Hodge Laplacian or the higher order adjacency. From this perspective, at first glance, simplicial attention seems a straightforward analogue of graph attention given. When the structure matrix is symmetric, such as the k-Hodge Laplacian, then the generalization is indeed relatively similar to the graph case. However, when considering the attention structure among simplices of different dimension, the attention mechanism becomes more complicated. Here we only define this difficult case.   \n",
    "\n",
    "To this end, let $A:C^{s}(\\mathcal{X})\\to C^{t}(\\mathcal{X})$ be a map. Assuming a fixed ordering of the simplices of the complex $\\mathcal{X}$, we will denote to the matrix induced by $A$ also by the same notation. Assuming the matrix $A$ is asymmetric, the simplicial attention network induced by $A$ is a cochain map $SimplAttenNet_A(W_s,W_t):C^{s}(\\mathcal{X},d_{s_{in}}) \\times C^{t}(\\mathcal{X},d_{t_{in}}) \\to C^{t}(\\mathcal{X},d_{t_{out}})$  defined via : \n",
    "\\begin{equation}\n",
    "(x_{s}^k,x_{t}^k) \\to x_{t}^{k+1} =  \\phi( (A \\odot att^{k}) * x_{s}^k * W_s )\n",
    "\\end{equation}                \n",
    "where $d_{in}$ is the feature dimension of input cochain $x_{i}^k$, $d_{out}$ dimension of the output cochain $x_{t}^{k+1}$, $W_s \\in \\mathbb{R}^{d_{s_{in}}\\times d_{t_{out}}} $ ,$W_t \\in \\mathbb{R}^{d_{t_{in}}\\times d_{s_{out}}} $ are trainable parameters and $att: C^{s}(\\mathcal{X})\\to C^{t}(\\mathcal{X}) $ is a simplicial attention matrix that has the same dimension as the matrix $A$ and is defined via $e^{l}_{st}= LeakyRelu((a^{l})^T (W_s x_{s}^k||W_t x_{t}^k) )$ and \n",
    "\n",
    "\\begin{equation}\n",
    "att_{s,t}^l =  \\frac{e_{st}^l}{ \\sum_{k \\in \\mathcal{N}_A(s) e_{sk}^l } }\n",
    "\\end{equation} \n",
    "\n",
    "here $a^{l} \\in \\mathbb{R}^{s_{out}+t_{out}} $  is a trainable parameter and $\\mathcal{N}_A(s)$ represent the neighboring structure of the simplex $s$ with respect to the matrix $A$. One may think about this structure as being the simplices $t$ that are nonzero in the column $s$ of the matrix $A$. \n",
    "\n",
    "Multiple things that one should notice about the above definition. First, the matrix $att$ is asymmetric and only the attention $att_{st}$ is recorded but not $att_{ts}$. The revesed attention matrix has to be computed seperatly. The reversed attention matrix has the same dimensions as the $A^T$, the tranpose of the operator $A$, and we will denote it by $\\bar{att}$. With this, the update equation becomes :\n",
    "\n",
    "\\begin{equation}\n",
    "(x_{s}^k,x_{t}^k) \\to x_{s}^{k+1} =  \\phi( (A^T \\odot \\bar{att}^{k}) * x_{t}^k * W_t )\n",
    "\\end{equation}  \n",
    "Note that when we perform this computation care must be taken when computing $e^l_{ts}$. Namely,\n",
    "\n",
    "$e^{l}_{ts}= LeakyRelu((rev(a^{l}))^T [W_t x_{t}^k||W_s x_{s}^k])$, where $rev(a^{l})= ( a^l[:t_{out}]||a^l[t_{out}:] ) $\n",
    "\n",
    "Our implementation below outputs both $x_{s}^{k+1}$ and $x_{t}^{k+1}$ when the input operator is asymetric.\n",
    "\n",
    "Now lets see how to define the above model using our package.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a simple simplicial complex\n",
    "\n",
    "Lets start by defining a very simple simplicial complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplices = [(0, 1, 2), (1, 2, 3), (2, 3), (1, 2, 4), (5, 3), (0, 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HL = SimplicialComplex(simplices)\n",
    "\n",
    "N0 = len(HL.n_faces(0))\n",
    "N1 = len(HL.n_faces(1))\n",
    "N2 = len(HL.n_faces(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the operators : \n",
    "We can get the operators defined on the complex we started above easily using the following function :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing the boundary matrices..\n",
      "\n",
      "computing the Hodge Laplacians matrices..\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py:125: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Adj0, Adj1, Coadj1, Coadj2, L0, L1, L2, B1, B2, B1T, B2T = mesh_2_operators(simplices,signed=False,\n",
    "                                                                          norm_method=None,\n",
    "                                                                           output_type=\"torch\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the data \n",
    "\n",
    "Lets generate some random data that lives on the complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random input data\n",
    "import torch\n",
    "\n",
    "feature_input_space_dim = 3\n",
    "batch_size = 10\n",
    "x_v = torch.rand(N0, 3)  # cochain on the nodes\n",
    "x_e = torch.rand(N1, 5)  # cochain on the edges\n",
    "x_f = torch.rand(N2, 3)  # cochain on the faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "In our implementation of the network above, we did a dense and a sparse implementations. To see how this works, we imported the functions SimplicialAttentionLayer and SparseSimplicialAttentionLayer above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 20])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a model :\n",
    "\n",
    "model_dense_arrays = HigherOrderAttentionLayer(\n",
    "    source_in_features=5,\n",
    "    target_in_features=3,\n",
    "    source_out_features=20,\n",
    "    target_out_features=20,\n",
    ")\n",
    "\n",
    "# infer :\n",
    "out_e, _ = model_dense_arrays(x_e, None, L1.to_dense())\n",
    "\n",
    "out_e.shape\n",
    "\n",
    "# the above model works with dense matrix, so things might be slow when the complex get larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse models are also supported :\n",
    "model = SparseHigherOrderAttentionLayer(\n",
    "    source_in_features=5,\n",
    "    target_in_features=3,\n",
    "    source_out_features=20,\n",
    "    target_out_features=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferance is similar to before\n",
    "v_out_s, out_e = model(\n",
    "    hs=x_e, ht=x_v, operator_list=B1.coalesce().indices(), operator_symmetry=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case the operator is symmetric, the second tensor should be empty in this case.\n",
    "out_e_2, _ = model(\n",
    "    hs=x_e, ht=None, operator_list=L1.coalesce().indices(), operator_symmetry=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi head attention can also be defined in the same way\n",
    "\n",
    "multihead_model = MultiHeadHigherOrderAttentionClassifer(\n",
    "    source_in_features=5,\n",
    "    target_in_features=3,\n",
    "    source_out_features=20,\n",
    "    target_out_features=20,\n",
    "    num_heads=3,\n",
    "    source_n_classes=3,\n",
    "    target_n_classes=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh_f_out, mh_e_out = multihead_model(x_e, x_f, B2.t().to_dense())\n",
    "mh_e_out2, _ = multihead_model(x_e, None, L1.to_dense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mh_e_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_model_sp = SpMultiHeadHigherOrderAttentionClassifer(\n",
    "    source_in_features=5,\n",
    "    target_in_features=3,\n",
    "    source_out_features=20,\n",
    "    target_out_features=20,\n",
    "    num_heads=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh_v_out_sp, mh_e_out_sp = multihead_model_sp(\n",
    "    x_e, x_f, B2.t().coalesce().indices(), False\n",
    ")\n",
    "mh_e_out2_sp, _ = multihead_model_sp(x_e, None, L1.coalesce().indices(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 5])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mh_e_out2_sp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
